# Q4.23 사전 랭킹 모델 최적화

> [!Tip]
>
> 🙋  많은 후보를 사전 랭킹해야 한다면 모델을 어떻게 최적화하나요?

-   투 타워 모델은 서빙 시 추천 후보에 효율적으로 스코어링할 수 있어 사전 랭킹에 널리 사용함
    -   사용자 임베딩과 후보 임베딩을 생성하고 내적을 수행하는 데 한 자릿수 밀리초 단위의 latency를 가짐
    -   보다 최적화하기 위해 임베딩을 캐싱하거나 사전 계산할 수 있음
        -   캐싱
            -   사용자 임베딩을 대규모 캐시에 저장하거나, 아이템 임베딩은 후보 생성 이전에라도 요청 처리 초기에 계산할 수 있음
        -   사전 계산 기법
            -   사용자 임베딩과 후보 임베딩을 전부 오프라인으로 미리 계산
            -   모델 업데이트에 신경을 많이 써야 한다는 단점이 있음
            -   임베딩 사전 계산 주기만큼 임베딩의 업데이트가 많이 늦어질 수 있음



-   투 타워 모델의 정확도 향상을 위해 DNN 사용
    -   일반적으로 DNN이 더 느리므로 아래 방법으로 최적화를 진행함

### 프로세스의 모든 단계에서 병렬성 추구

-   랭킹 요청을 여러 쿼리로 분할
-   [샤딩](https://aws.amazon.com/ko/what-is/database-sharding/)<sup>Sharding</sup>
    -   효율적인 캐시 전략을 유지하며 랭킹 요청을 분산하는 방법
-   피처를 동시에 가져오거나 feature hydration에 필요한 네트워크 호출을 최소화하는 것
-   배치 크기를 조정해 예측을 병렬로 실행할 수 있음

### 입력 최적화

-   피처를 연결해 밀도를 높임
-   학습 중 사용자 피처와 후보 피처에 dense한 임베딩 표현을 저장해 피처 압축 가능
    -   서빙할 때는 임베딩만 있으면 되기 때문에 모델의 내부 레이어에 직접 입력하면 됨
    -   알리바바의 COLD 모델에서는 피처가 캐시 친화적인 열 기반 형식으로 표현됨

### 여러 수준에서 피처 캐싱

-   Feature hydration latency를 최소화하기 위해 여러 수준에서 피처를 캐시해 latency와 cache hit rate 간 균형을 유지
    -   사용자 피처의 feature hydration은 추천 후보 생성 이전에 일어남
        -   다른 관련 작업에 지장을 주지 않는 방식<sup>non-blocking</sup>으로 교차 피처를 prefetch 할 수 있음
        -   모든 후보에 대해 피처를 가져올 수 없으므로 상위 $k$ 개 후보의 피처만 가져오고 나머지 후보의 교차 피처는 후보 생성 후 backfill할 수 있음
            -   Latency가 중요한 경우 포기할 수 있지만 정확도가 낮아질 수 있음

### 모델 아키텍처 최적화

-   정확성을 유지하면서 모델 크기와 계산 복잡성을 줄이는 최적화도 가능함
    -   양자화, pruning, low-rank factorization 등 사용
    -   알리바바 COLD 모델에 도입된 SE 블록과 같은 효율적인 모델 아키텍처 적용 가능

### Three-way Split Network

-   위 개념들과 투 타워 모델을 결합하는 아이디어

### 위 방법이 다 안될 때

-   모델 점수를 캐시하거나 오프라인으로 모두 미리 계산할 수 있음
    -   점수 캐시에 상당한 스토리지 비용이 발생할 수 있음
    -   TTL<sup>Time-To-Live</sup>이 낮다면 만족스러운 cache hit rate이 나오지 않음
    -   오프라인으로 점수를 미리 계산할 때 데이터 분포의 변화를 따라가지 못하는 현상이 발생함