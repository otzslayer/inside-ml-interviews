# Q4.12 위치 편향 완화

> [!Tip]
>
> 🙋  위치 편향을 어떻게 완화하나요?

### 위치를 무작위로 섞기

-   아이템 위치를 무작위로 섞음
    -   모델은 위치와 관계 없이 관련성 높은 아이템을 찾아내는 방법을 학습할 수 있음
    -   사용자 경험을 저하시킬 수 있음
    -   *   실제로 써봤지만 관련성 없는 아이템으로만 추천 결과가 구성되는 리스크가 있었음*

### 위치 피처 추가하기

-   위치 피처(아이템의 목록 내 위치)를 모델에 통합할 수 있음
    -   [Chapelle, Olivier, and Anne Ya Zhang. "A Dynamic Bayesian Network Click Model for Web Search Ranking." (2009).](https://web-archive.southampton.ac.uk/www2009.eprints.org/1/2/clicks_chapelle.pdf)
    -   Zinkevich, M. (2023, January 5). Rules of Machine Learning. Google Developers. Retrieved June 23, 2023, from https://developers.google.com/machine-learning/guides/rules-of-ml#rule_36_avoid_feedback_loops_with_positional_features
    -   위치 편향의 영향을 포착하는 데 도움이 됨
-   프로덕션에서 서빙할 때는 모델에 입력될 아이템의 위치를 모두 1로 설정해 해당 피처의 영향을 없앰

### 위치 편향을 모델링하기

-   아이템 확인<sup>examination</sup>과 관련성을 별도로 모델링하는 위치 편향 모델을 개발하고 이를 이용해 표준화<sup>normalization</sup> 또는 정규화<sup>regularization</sup>를 함
    -   확인이란 사용자가 아이템을 클릭하는 등 상호 작용하는 것만을 의미
    -   관련성은 아이템의 순수한 가치를 의미
    -   아이템 확인은 위치에 의존하고 관련성은 맥락과 아이템에 의존한다고 가정함
-   위치 편향은 전역적으로 모델링되거나(모든 쿼리에 대해 동일), 쿼리 유형에 따라 분할되거나, 일반화(위치 편향을 예측하도록 모델 학습)될 수 있음
    -   그 후 위치 편향의 효과를 포착하기 위해 역성향<sup>inverse propensity</sup> 가중 점수를 최적화하도록 랭킹 모델 학습
-   참고 사례
    -   마이크로소프트
        -   [Richardson, Matthew, Ewa Dominowska, and Robert Ragno. "Predicting clicks: estimating the click-through rate for new ads." *Proceedings of the 16th international conference on World Wide Web*. 2007.](https://www2007.cpsc.ucalgary.ca/papers/paper784.pdf)
        -   [Craswell, Nick, et al. "An experimental comparison of click position-bias models." *Proceedings of the 2008 international conference on web search and data mining*. 2008.](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=13d72ef522b405c18f7d228c5744687609b4c3a4)
    -   트립 어드바이저
        -   [Li, Yinxiao. "Handling position bias for unbiased learning to rank in hotels search." *arXiv preprint arXiv:2002.12528* (2020).](https://arxiv.org/pdf/2002.12528)
    -   구글
        -   [Wang, Xuanhui, et al. "Position bias estimation for unbiased learning to rank in personal search." *Proceedings of the eleventh ACM international conference on web search and data mining*. 2018.](https://dl.acm.org/doi/pdf/10.1145/3159652.3159732)
        -   [Wang, Xuanhui, et al. "Learning to rank with selection bias in personal search." *Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval*. 2016.](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/45286.pdf)

### 다중 작업 모델

-   Wide & Deep 모델이라고 가정
    -   위치 편향에 기여하는 피처를 다중 작업<sup>multi-task</sup> 랭킹 모델의 와이드 요소로 입력
    -   각 작업은 랭킹의 다양한 목표에 맞게 최적화됨
    -   각 작업에서 위치 편향의 효과를 모델이 파악해냄으로써 성향<sup>propensity</sup> 점수를 얻기 위해 위치를 무작위로 섞는 실험을 할 필요가 없음
-   유튜브 사례
    -   [Zhao, Zhe, et al. "Recommending what video to watch next: a multitask ranking system." *Proceedings of the 13th ACM Conference on Recommender Systems*. 2019.](https://daiwk.github.io/assets/youtube-multitask.pdf)

### 적대적 학습

-   학습 데이터로부터 아이템의 위치를 예측하는 보조<sup>auxillary</sup> 작업을 정의
    -   역전파 단계에서는 모델 예측에 대한 위치 피처의 영향을 감소하기 위해 모델에 전달되는 경사의 부호를 바꿈<sup>negate</sup>
-   일반적으로 다음 작업에 많이 활용됨
    -   도메인 적응
        -   [Tzeng, Eric, et al. "Adversarial discriminative domain adaptation." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2017.](https://velog.io/@yetsyl0705/2017-CVPR-Adversarial-Discriminative-Domain-Adaptation-3867%ED%9A%8C-%EC%9D%B8%EC%9A%A9)
    -   머신러닝 공정성<sup>machine learning fairness</sup>
        -   [Beutel, Alex, et al. "Data decisions and theoretical implications when adversarially learning fair representations." *arXiv preprint arXiv:1707.00075* (2017).](https://arxiv.org/pdf/1707.00075)