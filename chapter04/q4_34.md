# Q4.34 캐싱

> [!Tip]
>
> 🙋  서빙 시스템에서 무엇을 어디에 캐시할 수 있나요?

### 각 사용자의 상위 k개의 아이템 캐싱

-   전체 후보를 가져와서 랭킹을 할 필요 없이 각 사용자의 상위 k개 아이템을 캐시에 넣어둠
-   메타는 FeedState를 사용해 서빙 전에 랭킹 점수를 조정해 캐시된 추천을 최신 상태로 유지
    -   [Khandelwal, Hitesh, et al. "Jointly Optimize Capacity, Latency and Engagement in Large-scale Recommendation Systems." *Proceedings of the 15th ACM Conference on Recommender Systems*. 2021.](https://research.facebook.com/publications/jointly-optimize-capacity-latency-and-engagement-in-large-scale-recommendation-systems/)

### 임베딩 캐싱

-   임베딩을 캐시해 Approximation Nearest Neighbor (ANN)에 활용할 수 있음
-   마이크로소프트의 SPANN 기법은 대규모 후보 공간에 대한 캐싱을 효율적으로 구현함
    -   [Chen, Qi, et al. "Spann: Highly-efficient billion-scale approximate nearest neighborhood search." *Advances in Neural Information Processing Systems* 34 (2021): 5199-5212.](https://proceedings.neurips.cc/paper_files/paper/2021/file/299dc35e747eb77177d9cea10a802da2-Paper.pdf)

### 피처 캐싱

-   랭킹 함수 호출<sup>call/request</sup> 간에 재사용될 가능성이 큰 후보 피처를 여러 수준에서 캐시함
    -   로컬 메모리<sup>in-memory</sup> 나 데이터 센터 수준
    -   후보 랭킹 함수가 요청될 때마다 네트워크 호출을 통해 피처를 가져올 필요가 없음 -> 시스템 성능 향상

### 예측 점수 캐싱

-   사용자가 요청할 때마다 다시 계산할 필요가 없도록 쌍별 예측 점수를 짧은 TTL을 가지는 캐시에 저장
    -   시스템 성능 향상 및 모델 서버 부하 감소



<img src="https://i.imgur.com/FnRw6lF.png" style="zoom:50%;" />

-   참고 사항
    -   단순화를 위해 사전 랭킹 단계 생략
    -   점선 화살표: 조건부 연결
        -   캐시에서 후보 피처를 검색할 수 없으면 피처 스토어에서 비동기식으로 가져옴
        -   투 타워 모델 아키텍처의 경우 게이트웨이 서비스가 유사도 지표를 독립적으로 계산할 수 있음
            -   모델 서버를 주요 경로<sup>critical path</sup>에서 호출할 필요 없음