# Q6.16 서빙 시 성능 저하

**🙋 질문 : 온라인 모델 성능이 오프라인 지표에 비해 떨어지는 경우에 어떻게 해결하나요?**

## 배포 된 모델의 성능 감지 방법

- 모델의 예측 결과와 실측 레이블(사용자가 실제로 반응한 결과)을 비교하여 모델을 오프라인으로 일괄 평가하고 학습 중에 관찰된 지표와 상당한 편차를 보이는지 확인
- 테스트 데이터를 모델 서버에 요청으로 보내어 얻은 결과를 오프라인 결과와 비교하여 지표의 차이 확인

## 서빙 시 모델 성능 문제 해결 방법

#### - 데이터 확인

- 학습과 서빙에 사용된 데이터 동일 또는 유사한지 확인
  - 이를 위해 오프라인에서도 검증할 수 있도록 모델 예측값, 수화된 피처 기록
  - 학습 데이터와 비교해 수화된 피처에 데이터 누출/누락 또는 드리프트가 있는지 확인
  - 오프라인에서 수화된 피처를 사용해 동일하게 모델 평가/비교

#### - 모델 메타데이터 로깅

- 서빙되는 모델의 이름, 버전을 확인하거나 모델 스냅샷을 저장해 모델을 구별할 수 있도록 함

#### - 모니터링 추가

- 오래된 캐시, 캐시 누락, 예측 오류(타임아웃)에 관한 지표를 추가
- 캐시가 오래되면 잘못된 데이터를 반환해 모델 성능에 부정적 영향을 미칠 수 있음
- 캐시 누락률이 높으면 응답 시간 지연이 발생하거나 타임아웃이 늘어나서 피처값이 누락될 수 있음

#### - 단위 테스트 추가

- 특정 케이스에 대한 단위 테스트를 추가
  - 입력 형식이 올바른지, 필요한 피처 수화가 올바르게 적용됐는지 확인
  - 데이터 입력 처리 기술이 제대로 동작하는지 확인

#### - 디버거 추가

- 명령줄 또는 웹 기반 도구 형태로 디버거 개발
- 요청에 대한 중간 형식 덤프, 수화된 피처를 표시할 수 있어야 함
  - 누락된 값이나 캐싱 정책 문제를 찾아내는 데 유용
- 모델의 예측값이 예상 범위 내로 생성되는지 확인
- 여러 요청을 처리할 때 예측결과가 올바르게 딜리버리 되는지 확인

#### - 캐싱 및 타임아웃 비활성화

- 스테이징에서 캐싱을 비활성화하고 타임아웃을 늘리거나 비활성화해 문제를 분리해서 보는 방법

#### - 평가 지표 확인

- 학습 중에 사용한 평가 지표와 모델 서버를 평가할 때 사용한 평가 지표가 동일한지 확인
- 평가 지표가 주어진 문제 적합한 지표인지 확인
- 시간이 지남에 따라 오프라인 모델 성능과 온라인 모델 성능에 어느정도 격차가 발생할 수 있음을 인식
