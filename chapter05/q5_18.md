# Q5.18 텍스트 생성 모델 구축

>   [!Tip]
>
>   🙋 추출적 문서 요약을 설명하세요.

- 추출적 기법에는 일련의 분류기가 필요하며 프로세스는 다음과 같음
1. 분류기가 관련 문장을 식별함
2. 유사도 기준(ex. 잠재 표현)을 기반으로 중복 문장을 클러스터링해 중복 제거
3. 중요성이나 텍스트 내 위치 등의 기준에 따라 문장에 순서를 지정함
4. 쌍별 분류를 사용해 상호 참조 해결을 수행하여 텍스트에서 동일한 엔터티에 대한 언급을 모아 그룹화함


>   [!Tip]
>
>   🙋 추상적 문서 요약을 설명하세요.

- 초기 기법은 통계적 기계 번역(SMT)에서 영감을 받은 노이지 채널 접근 방식에 기반을 둠
	- 콘텐츠 선택과 표층 구현<sup>surface realization</sup>으로 구성
	- 콘텐츠 선택
		- 특정 토큰이 요약에 나타날 확률에 초점을 맞춤
	- 표층 구현
		- 선택한 토큰을 의미 있는 방식으로 배열하는데 중점
- 신경망 기법
	- RNN
		- [Nallapati, Ramesh, et al. "Abstractive text summarization using sequence-to-sequence rnns and beyond." _arXiv preprint arXiv:1602.06023_ (2016).](https://aclanthology.org/K16-1028.pdf)
	- LSTM
		-  [Chopra, Sumit, Michael Auli, and Alexander M. Rush. "Abstractive sentence summarization with attentive recurrent neural networks." _Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies_. 2016.](https://aclanthology.org/N16-1012.pdf)
	- CNN
		- 과거 전체의 정보를 저장하는 RNN과 달리 고정 길이의 맥락 정보를 표현한다는 장점이 있음
			- 모델링할 종속성 범위를 보다 정확하게 제어함
		- 계층적 구조를 통해 입력 시퀀스의 장거리 종속성을 RNN보다 더 짧은 경로로 포착함
		- ConvS2S
			- [Gehring, Jonas, et al. "Convolutional sequence to sequence learning." _International conference on machine learning_. PMLR, 2017.](http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf)
- LLM 기반
	- **T5<sup>Text-to-Text Transfer Transformer</sup>**
		- [Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer." _Journal of machine learning research_ 21.140 (2020): 1-67.](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)
		- 구글에서 개발한 LLM
		- 모든 언어 문제를 통일된 텍스트 대 텍스트 형식으로 변환하여 처리
		- 다양한 길이와 무작위 마스크 비율이 적용된 텍스트로 사전학습됨
			- 이런 사전 학습 방식을 통해 광범위한 자연어 처리 작업에 유연함
	- **BART<sup>Bidirection and Auto-Regressive Transformer</sup>**
		- [Lewis, M. "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension." _arXiv preprint arXiv:1910.13461_ (2019).](https://fq.pkwyx.com/default/https/aclanthology.org/2020.acl-main.703.pdf)
		- 페이스북 AI 리서치에서 소개한 LLM
		- 디노이징 오토인코더<sup>Denoising autoencoder</sup> 방식으로 학습
			- 손상된 버전에서 원본 입력 텍스트를 재구성
		- 사전 학습 단계에서 토큰 마스킹, 토큰 삭제, 문장 무작위 섞기 등 다양한 방법 사용
	- **PEGASUS**
		- 추상적 요약을 위해 특별히 설계된 LLM
			- 아키텍처가 BART와 동일하나 사전 학습 작업이 GSG<sup>Gap Sentences Generation</sup>으로 다름
				- 마스킹을 문장 단위로 적용한 다음 언어 모델이 나머지 문서 맥락을 기반으로 마스킹된 문장들을 생성하도록 학습
			- 마스킹할 문장을 선택하기 위해 정해진 수의 문장을 무작위로 선택하거나 처음 몇 개 문장을 선택하는 등 다양한 전략 사용