# Q5.18 í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ êµ¬ì¶•

>   [!Tip]
>
>   ğŸ™‹ ì¶”ì¶œì  ë¬¸ì„œ ìš”ì•½ì„ ì„¤ëª…í•˜ì„¸ìš”.

- ì¶”ì¶œì  ê¸°ë²•ì—ëŠ” ì¼ë ¨ì˜ ë¶„ë¥˜ê¸°ê°€ í•„ìš”í•˜ë©° í”„ë¡œì„¸ìŠ¤ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ
1. ë¶„ë¥˜ê¸°ê°€ ê´€ë ¨ ë¬¸ì¥ì„ ì‹ë³„í•¨
2. ìœ ì‚¬ë„ ê¸°ì¤€(ex. ì ì¬ í‘œí˜„)ì„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ë¬¸ì¥ì„ í´ëŸ¬ìŠ¤í„°ë§í•´ ì¤‘ë³µ ì œê±°
3. ì¤‘ìš”ì„±ì´ë‚˜ í…ìŠ¤íŠ¸ ë‚´ ìœ„ì¹˜ ë“±ì˜ ê¸°ì¤€ì— ë”°ë¼ ë¬¸ì¥ì— ìˆœì„œë¥¼ ì§€ì •í•¨
4. ìŒë³„ ë¶„ë¥˜ë¥¼ ì‚¬ìš©í•´ ìƒí˜¸ ì°¸ì¡° í•´ê²°ì„ ìˆ˜í–‰í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ ë™ì¼í•œ ì—”í„°í‹°ì— ëŒ€í•œ ì–¸ê¸‰ì„ ëª¨ì•„ ê·¸ë£¹í™”í•¨


>   [!Tip]
>
>   ğŸ™‹ ì¶”ìƒì  ë¬¸ì„œ ìš”ì•½ì„ ì„¤ëª…í•˜ì„¸ìš”.

- ì´ˆê¸° ê¸°ë²•ì€ í†µê³„ì  ê¸°ê³„ ë²ˆì—­(SMT)ì—ì„œ ì˜ê°ì„ ë°›ì€ ë…¸ì´ì§€ ì±„ë„ ì ‘ê·¼ ë°©ì‹ì— ê¸°ë°˜ì„ ë‘ 
	- ì½˜í…ì¸  ì„ íƒê³¼ í‘œì¸µ êµ¬í˜„<sup>surface realization</sup>ìœ¼ë¡œ êµ¬ì„±
	- ì½˜í…ì¸  ì„ íƒ
		- íŠ¹ì • í† í°ì´ ìš”ì•½ì— ë‚˜íƒ€ë‚  í™•ë¥ ì— ì´ˆì ì„ ë§ì¶¤
	- í‘œì¸µ êµ¬í˜„
		- ì„ íƒí•œ í† í°ì„ ì˜ë¯¸ ìˆëŠ” ë°©ì‹ìœ¼ë¡œ ë°°ì—´í•˜ëŠ”ë° ì¤‘ì 
- ì‹ ê²½ë§ ê¸°ë²•
	- RNN
		- [Nallapati, Ramesh, et al. "Abstractive text summarization using sequence-to-sequence rnns and beyond."Â _arXiv preprint arXiv:1602.06023_Â (2016).](https://aclanthology.org/K16-1028.pdf)
	- LSTM
		-  [Chopra, Sumit, Michael Auli, and Alexander M. Rush. "Abstractive sentence summarization with attentive recurrent neural networks."Â _Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies_. 2016.](https://aclanthology.org/N16-1012.pdf)
	- CNN
		- ê³¼ê±° ì „ì²´ì˜ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” RNNê³¼ ë‹¬ë¦¬ ê³ ì • ê¸¸ì´ì˜ ë§¥ë½ ì •ë³´ë¥¼ í‘œí˜„í•œë‹¤ëŠ” ì¥ì ì´ ìˆìŒ
			- ëª¨ë¸ë§í•  ì¢…ì†ì„± ë²”ìœ„ë¥¼ ë³´ë‹¤ ì •í™•í•˜ê²Œ ì œì–´í•¨
		- ê³„ì¸µì  êµ¬ì¡°ë¥¼ í†µí•´ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ì¥ê±°ë¦¬ ì¢…ì†ì„±ì„ RNNë³´ë‹¤ ë” ì§§ì€ ê²½ë¡œë¡œ í¬ì°©í•¨
		- ConvS2S
			- [Gehring, Jonas, et al. "Convolutional sequence to sequence learning."Â _International conference on machine learning_. PMLR, 2017.](http://proceedings.mlr.press/v70/gehring17a/gehring17a.pdf)
- LLM ê¸°ë°˜
	- **T5<sup>Text-to-Text Transfer Transformer</sup>**
		- [Raffel, Colin, et al. "Exploring the limits of transfer learning with a unified text-to-text transformer."Â _Journal of machine learning research_Â 21.140 (2020): 1-67.](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf)
		- êµ¬ê¸€ì—ì„œ ê°œë°œí•œ LLM
		- ëª¨ë“  ì–¸ì–´ ë¬¸ì œë¥¼ í†µì¼ëœ í…ìŠ¤íŠ¸ ëŒ€ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
		- ë‹¤ì–‘í•œ ê¸¸ì´ì™€ ë¬´ì‘ìœ„ ë§ˆìŠ¤í¬ ë¹„ìœ¨ì´ ì ìš©ëœ í…ìŠ¤íŠ¸ë¡œ ì‚¬ì „í•™ìŠµë¨
			- ì´ëŸ° ì‚¬ì „ í•™ìŠµ ë°©ì‹ì„ í†µí•´ ê´‘ë²”ìœ„í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ìœ ì—°í•¨
	- **BART<sup>Bidirection and Auto-Regressive Transformer</sup>**
		- [Lewis, M. "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension."Â _arXiv preprint arXiv:1910.13461_Â (2019).](https://fq.pkwyx.com/default/https/aclanthology.org/2020.acl-main.703.pdf)
		- í˜ì´ìŠ¤ë¶ AI ë¦¬ì„œì¹˜ì—ì„œ ì†Œê°œí•œ LLM
		- ë””ë…¸ì´ì§• ì˜¤í† ì¸ì½”ë”<sup>Denoising autoencoder</sup> ë°©ì‹ìœ¼ë¡œ í•™ìŠµ
			- ì†ìƒëœ ë²„ì „ì—ì„œ ì›ë³¸ ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì¬êµ¬ì„±
		- ì‚¬ì „ í•™ìŠµ ë‹¨ê³„ì—ì„œ í† í° ë§ˆìŠ¤í‚¹, í† í° ì‚­ì œ, ë¬¸ì¥ ë¬´ì‘ìœ„ ì„ê¸° ë“± ë‹¤ì–‘í•œ ë°©ë²• ì‚¬ìš©
	- **PEGASUS**
		- ì¶”ìƒì  ìš”ì•½ì„ ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ëœ LLM
			- ì•„í‚¤í…ì²˜ê°€ BARTì™€ ë™ì¼í•˜ë‚˜ ì‚¬ì „ í•™ìŠµ ì‘ì—…ì´ GSG<sup>Gap Sentences Generation</sup>ìœ¼ë¡œ ë‹¤ë¦„
				- ë§ˆìŠ¤í‚¹ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì ìš©í•œ ë‹¤ìŒ ì–¸ì–´ ëª¨ë¸ì´ ë‚˜ë¨¸ì§€ ë¬¸ì„œ ë§¥ë½ì„ ê¸°ë°˜ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ëœ ë¬¸ì¥ë“¤ì„ ìƒì„±í•˜ë„ë¡ í•™ìŠµ
			- ë§ˆìŠ¤í‚¹í•  ë¬¸ì¥ì„ ì„ íƒí•˜ê¸° ìœ„í•´ ì •í•´ì§„ ìˆ˜ì˜ ë¬¸ì¥ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ê±°ë‚˜ ì²˜ìŒ ëª‡ ê°œ ë¬¸ì¥ì„ ì„ íƒí•˜ëŠ” ë“± ë‹¤ì–‘í•œ ì „ëµ ì‚¬ìš©