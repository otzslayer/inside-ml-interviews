# Q5.3 토픽 모델링 기법

>   [!Tip]
>
>   🙋 뉴스 기사에 어떤 토픽들이 들어 있는지 식별하는 ML 시스템을 구축합니다. 토픽 모델링 기법에는 어떤 것이 있나요?

### 단어 기반 기술

-   단어 임베딩 또는 단어 빈도 기반 방법을 사용해 특정 토픽에 관한 단어가 있는지 확인
    -   구현이 간단하고 빠르지만 토픽 간의 복잡한 관계를 포착하지는 못함
    -   [Meddeb, Amna, and Lotfi Ben Romdhane. "Using topic modeling and word embedding for topic extraction in Twitter." *Procedia Computer Science* 207 (2022): 790-799.](https://www.sciencedirect.com/science/article/pii/S1877050922010158/pdf?md5=a23ab885400883086f7a5fade38383e2&pid=1-s2.0-S1877050922010158-main.pdf)
-   구현 방법
    1.   키워드 추출
         -   각 문서에서 가장 두드러진 키워드 추출
         -   TF-IDF, Chunking, 토픽 단어와 같은 다양한 기법으로 수행
    2.   단어 임베딩
         -   추출한 각 키워드 또는 문구를 GloVe와 같은 단어 임베딩을 사용해 낮은 차원 벡터로 표현
             -   [Wu, Yongliang, Shuliang Zhao, and Wenbin Li. "Phrase2Vec: phrase embedding based on parsing." *Information Sciences* 517 (2020): 100-127.](https://sci-hub.se/downloads/2020-07-19/c7/wu2019.pdf)
             -   [Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. "Glove: Global vectors for word representation." *Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)*. 2014.](https://aclanthology.org/D14-1162.pdf)
    3.   임베딩 유사도
         -   키워드와 토픽 시드 단어를 임베딩 거리 기반으로 매칭
         -   행렬 분해와 같은 기술을 사용해 공유 임베딩 공간을 학습시키면 키워드 임베딩과 토픽 임베딩의 유사도를 직접 비교할 수 있음

### 행렬 분해

-   단어 동시 발생<sup>Word co-occurrences</sup> 또는 단어-문서 빈도<sup>term-document counts</sup> 행렬을 잠재 토픽 및 해당 가중치의 저차원 행렬로 인수 분해
    -   잠재 의미 분석<sup>Latent Semantic Analysis</sup>(LSA)가 널리 사용됨
        -   Latent semantic analysis. (n.d.). Wikipedia. Retrieved September 6, 2024, from https://en.wikipedia.org/wiki/Latent_semantic_analysis
    -   비음수 행렬 분해<sup>Non-negative Matrix Factorization</sup>(NMF)는 행렬 원소가 음수가 아니어야 한다는 제약을 부과해 해석 가능성이 큰 결과를 얻음
        -   결과로 얻는 벡터를 덧셈이 가능한 문서 토픽 요소로 해석할 수 있기 때문
        -   [Arora, Sanjeev, Rong Ge, and Ankur Moitra. "Learning topic models--going beyond SVD." *2012 IEEE 53rd annual symposium on foundations of computer science*. IEEE, 2012.](https://arxiv.org/pdf/1204.1956)

### 확률적 토픽 모델링

-   각 문서를 토픽들의 혼합으로 모델링하고 각 토픽을 단어에 대한 분포로 모델링
-   잠재 디리클레 할당<sup>Latent Dirichlet Allocation</sup>(LDA)를 주로 사용함
    -   토픽 분포 추정을 위해 베이지안 추론 사용
    -   2단계 프로세스
        -   디리클레 분포에서 토픽 확률 집합을 추출한 다음 해당 토픽에서 단어 집합 추출
        -   관찰된 데이터의 가능도를 최대화하는 반복 작업을 수행해 각 문서에 대해 가장 가능성이 높은 토픽 분포 추론
    -   [Blei, David M., Andrew Y. Ng, and Michael I. Jordan. "Latent dirichlet allocation." *Journal of machine Learning research* 3.Jan (2003): 993-1022.](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?ref=http://githubhelp.com)
-   계층적 디리클레 과정<sup>Hierarchical Dirichlet Process</sup>(HDP)
    -   LDA의 확장판
    -   데이터에서 무한한 개수의 토픽 추론 가능
    -   [Teh, Yee, et al. "Sharing clusters among related groups: Hierarchical Dirichlet processes." *Advances in neural information processing systems* 17 (2004).](https://proceedings.neurips.cc/paper_files/paper/2004/file/fb4ab556bc42d6f0ee0f9e24ec4d1af0-Paper.pdf)
-   Correlated Topic Models (CTM)
    -   토픽의 상관관계를 허용하는 방식으로 LDA 확장
    -   토픽이 공유 공분산 행렬을 사용하는 다변량 가우스 분포에서 생성된다고 가정함
    -   [Blei, David, and John Lafferty. "Correlated topic models." *Advances in neural information processing systems* 18 (2006): 147.](https://www.cs.cmu.edu/afs/cs/usr/lafferty/www/pub/ctm.pdf)

### 신경망 토픽 모델

-   신경망을 사용해 잠재 토픽을 발견해 문서를 토픽 분포로 표현
-   확률적 토픽 모델링에 비해 개선점이 있음
    -   토픽 및 단어 분포를 확률 벡터 또는 임베딩으로 표현할 수 있음 -> LDA도 갖고 있음
    -   대규모 corpus로 확장하고 GPU를 활용하기에 효율적
    -   텍스트 생성과 문서 요약 등 공동 학습 목적을 위해 다른 신경망과 결합하기 용이
        -   [Wang, Wenlin, et al. "Topic-guided variational autoencoders for text generation." *arXiv preprint arXiv:1903.07137* (2019).](https://arxiv.org/pdf/1903.07137)
        -   [Cui, Peng, Le Hu, and Yuanchao Liu. "Enhancing extractive text summarization with topic-aware graph neural networks." *arXiv preprint arXiv:2010.06253* (2020).](https://arxiv.org/pdf/2010.06253)
-   주로 Variational Auto-Encoder (VAE)를 기반으로 함
    -   입력 데이터를 잠재 공간에 매핑하고 디코더는 잠재 표현을 다시 입력 공간에 매핑함
    -   NTM의 맥락에서 VAE가 토픽을 직접 모델링하지 않고 문서의 암시적<sup>implicit</sup>인 표현을 학습하는데 사용되거나 토픽 할당 및 해당 토픽 내의 단어 분포를 명시적으로 모델링할 수 있음
    -   디리클레 분포가 쉬운 최적화를 위해 가우시안 등으로 대체되기도 함
        -   [Miao, Yishu, Edward Grefenstette, and Phil Blunsom. "Discovering discrete latent topics with neural variational inference." *International conference on machine learning*. PMLR, 2017.](https://proceedings.mlr.press/v70/miao17a/miao17a.pdf)
    -   LDA와 같은 확률적 토픽 모델링보다 좋은 성능을 보임
        -   [Miao, Yishu, Lei Yu, and Phil Blunsom. "Neural variational inference for text processing." *International conference on machine learning*. PMLR, 2016.](https://proceedings.mlr.press/v48/miao16.pdf)

<img src="https://i.imgur.com/h7iGo2n.png" style="zoom:50%;" />