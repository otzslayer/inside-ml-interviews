# Q5.2 감성 분석

>   [!Tip]
>
>   🙋 주어진 텍스트가 긍정적인지 혹은 부정적인지 판단하는 ML 시스템을 설계합니다.

-   감성 분석에는 다음을 포함한 여러 가지 과제가 있음
    -   부정
    -   풍자
    -   비교
    -   반전
    -   긍정적 부정
-   감성 분석에 대한 접근 방식
    -   규칙 기반
        -   텍스트에 감성을 할당하기 위해 일련의 규칙 정의
            -   텍스트에 '행복<sup>happy</sup>', '흥분<sup>excited</sup>' 이라는 단어가 포함되어 있으면 긍정
            -   '슬픔<sup>sad</sup>', '실망<sup>disappointed</sup>'이 포함되어 있으면 부정
            -   부정에 대한 규칙 추가도 가능
                -   [Das, Sanjiv Ranjan, and Mike Y. Chen. "Yahoo! for Amazon: Sentiment parsing from small talk on the web." *For Amazon: Sentiment Parsing from Small Talk on the Web (August 5, 2001). EFA* (2001).](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=89af734dad7645ca3f3a7717443a1e49f37964bc)
        -   구현이 간단하고 변경이 쉬우며 풍자나 아이러니 같은 미묘한 차이 식별도 효과적
        -   시간이 많이 걸리고, 도메인 전문 지식이 필요하며 모델이 불안정<sup>brittle</sup>하고 일반화가 어려움 
    -   어휘 기반
        -   긍정적이거나 부정적인 감성과 관련해 사전 정의한 어휘집 사용
            -   텍스트 감성 점수는 어휘집 내 텍스트의 단어 점수를 합산해 계산됨
                -   모호성 낮은 단어에 더 낮은 가중치 할당
                -   명확한 감정 단어 간의 점별 상호 정보량<sup>Pointwise Mutual Information (PMI)</sup>을 계산하는 방식도 있음
                    -   $PMI(x, y) = \log_2\frac{P(x, y)}{P(x) \cdot P(y)}$
                    -   [Turney, Peter D. "Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews." *arXiv preprint cs/0212032* (2002).](https://arxiv.org/pdf/cs/0212032)
            -   속도가 빨라 텍스트 입력량이 많을 때 효과적임
            -   부정이나 아이러니와 같은 까다로운 케이스를 처리하기 어려움
                -   어휘 목록의 범위와 품질에 영향을 받음
    -   비신경망 모델
        -   감성 레이블이 지정된 텍스트 데이터셋으로 ML 모델 학습
            -   SVM, 로지스틱 회귀, 토픽 모델 등 여러 가지가 있음
                -   [Mcauliffe, Jon, and David Blei. "Supervised topic models." *Advances in neural information processing systems* 20 (2007).](https://proceedings.neurips.cc/paper/2007/file/d56b9fc4b0f1be8871f5e1c40c0067e7-Paper.pdf)
            -   대부분의 방법론에서 입력 텍스트를 Bag-of-Words (BOW)으로 표현
        -   규칙이나 어휘 목록을 만들 필요 없이 감성을 식별할 수 있고 일반화가 잘 됨
        -   풍자와 같은 미묘한 차이를 다루기 어려우며 레이블이 지정된 학습 데이터가 많이 필요함
    -   신경망
        -   LSTM, 트랜스포머 인코더 등 단어 간의 맥락과 관계를 잘 포착할 수 있는 모델을 사용
            -   모델이 단어의 순서와 종속성을 고려할 수 있는데 이는 감성 분석에 특히 유용함 
    -   RoBERTa
        -   Robustly Optimized BERT pre-training Approach
            -   [Liu, Yinhan. "Roberta: A robustly optimized bert pretraining approach." *arXiv preprint arXiv:1907.11692* (2019).](https://www.cs.princeton.edu/~danqic/papers/roberta_paper.pdf)
            -   [Hartmann, Jochen, et al. "More than a feeling: Accuracy and application of sentiment analysis." *International Journal of Research in Marketing* 40.1 (2023): 75-87.](https://www.sciencedirect.com/science/article/pii/S0167811622000477)
        -   BERT의 변형
            -   BERT에 비해 더 많은 데이터를 더 오래 학습함
            -   개별 단어 간의 관계 모델링에 초점을 맞추기 위해 학습 목표 중 다음 문장 예측<sup>Next Sentence Prediction (NSP)</sup> 태스크를 제외함
            -   일반화 능력 향상을 위해 동적 마스킹<sup>dynamic masking</sup> 적용
                -   학습 중 배치마다 서로 다른 토큰을 무작위로 마스킹
            -   레이블이 없는 대량의 텍스트로 사전 학습하고 상대적으로 작은 크기의 감성 분석 데이터셋으로 파인튜닝
        -   풍자, 아이러니 등 복잡하고 미묘한 표현을 처리할 수 있고, 파인튜닝에 많은 데이터가 필요하지 않음
        -   계산 비용이 높고 해석 가능성이 제한됨