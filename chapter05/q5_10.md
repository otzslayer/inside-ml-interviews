# Q5.10 정보 추출 모델 구축

>   [!Tip]
>
>   🙋 재무 보고서 등에서 정보를 추출하는 모델을 구축합니다. 어떤 기법이 있나요?

### 규칙 기반 시스템

-   주어진 텍스트에서 특정 정보를 식별하고 추출하기 위한 일련의 규칙과 패턴을 만듦
    -   도메인 전문가가 규칙을 직접 작성하는 경우가 많음
    -   광범위한 텍스트에 적용할 수 있음
-   여전히 많은 IE 시스템에서 중요한 역할을 함
    -   ML 모델 성능이 부족한 부분이 있을 때 정교한 규칙을 추가해 개선

### 규칙 기반 하이브리드 접근 방식

-   규칙 기반 모델과 통계 모델을 결합
    -   정확성<sup>Precision</sup>과 적용 범위<sup>coverage</sup>를 향상
-   완전 통계적 접근 방식보다 유연성과 해석 가능성이 높음
-   도메인 전문 지식이 필요해 개발 및 유지 관리 비용이 더 높음

### 통계 모델

-   Hidden Markov Model (HMM)
    -   정보 추출에 적용된 초기 통계 모델 중 하나
    -   일련의 입력 토큰이 주어지면 결합 확률을 사용해 태그 시퀀스의 확률 모델링
    -   관찰된 토큰 정보만 사용하며 다른 피처는 고려하지 않음
-   Maximum Entropy Markov Model (MEMM)
    -   HMM의 확장
    -   로지스틱 회귀를 통해 방출<sup>emission</sup> 확률 분포를 조건부 모델링
    -   이를 통해 시퀀스 레이블링 작업에 추가 피처를 사용할 수 있음
        -   HMM에선 불가능
-   Conditional Random Field (CRF)
    -   MEMM의 확장
    -   레이블 편향 문제를 해결할 수 있음
        -   레이블 편향
            -   어떤 상태<sup>state</sup>가 엔트로피가 낮은 레이블 전환<sup>transition</sup>을 선택함으로써 관련성이 있는 피처를 결과적으로 무시하게 되는 현상
            -   시퀀스 전체에서 얻을 수 있는 정보를 사용하지 않고, 주어진 위치의 상태에서 가장 확률이 높은 레이블 전환만을 선택
        -   피처 함수를 사용해 입력 및 출력 시퀀스 간의 종속성 모델링

### 신경망 모델

-   CNN과 BiLSTM 자주 사용
-   CNN
    -   입력 텍스트에 컨볼루션 연산을 적용해 IE에 사용
    -   IE의 맥락에서 CNN은 슬라이딩 윈도우 접근 방식 사용
        -   한 번에 고정 크기의 토큰 윈도우에 컨볼루션을 적용해 각 윈도우에 대한 피처 맵 생성
        -   피처 맵은 Max-pooling 레이어를 통과해 가장 관련성이 높은 피처들을 추출
-   BiLSTM<sup>Bidirectional Long Short-Term Memory</sup>

### BiLSTM-CNN-CRF

<img src="https://i.imgur.com/fyKFmLB.png" style="zoom:50%;" />

-   위 모델들을 결합한 BiLSTM-CNN-CRF 방법론도 있음
    -   CNN이 각 단어를 문자 단위로 처리해 접두사와 같은 형태학적<sup>morphological</sup> 정보를 포착
        -   문자 수준의 단어 표현을 생성하는 데 사용
    -   BiLSTM은 문자 수준 단어 표현을 단어 임베딩과 연결하고 이를 정방향 및 역방향으로 처리해 입력 시퀀스에서 단어 간의 장거리 관계 포착
    -   CRF 레이어는 BiLSTM으로부터 문맥 정보를 담은 단어 표현을 받아서, 인접한 출력 레이블 간의 종속성을 고려해 최종 출력 레이블 시퀀스 생성
-   아키텍처의 변형도 많음
    -   BiLSTM과 CNN에서 병렬로 피처 추출하여 concatenate한 뒤 CRF에 제공
    -   CNN 없이 BiLSTM과 CRF만 사용
    -   CNN을 BERT와 같은 사전 학습 언어 모델로 대체
        -   [Devlin, Jacob. "Bert: Pre-training of deep bidirectional transformers for language understanding." *arXiv preprint arXiv:1810.04805* (2018).](https://arxiv.org/pdf/1810.04805)
    -   사전 학습된 BERT 인코더를 BiLSTM-CRF 아키텍처의 입력으로 사용하여 입력 텍스트에서 맥락 정보 포착
        -   [Liu, Shuang, et al. "Chinese named entity recognition method in history and culture field based on BERT." *International Journal of Computational Intelligence Systems* 14 (2021): 1-10.](https://link.springer.com/content/pdf/10.1007/s44196-021-00019-8.pdf)